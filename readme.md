# 游뱄 Zero-Shot Learning for Unitree G1 Locomotion & Manipulation

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/Python-3.9%2B-blue)](https://www.python.org)
[![MuJoCo 3.0](https://img.shields.io/badge/MuJoCo-3.0-orange)](https://mujoco.org)

> **Implementaci칩n pr치ctica de Zero-Shot Learning para el robot humanoide Unitree G1, permitiendo ejecutar tareas de locomoci칩n y manipulaci칩n mediante instrucciones en lenguaje natural sin entrenamiento espec칤fico.**

## 游 Visi칩n
Este proyecto implementa un sistema de **aprendizaje cero disparo (ZSL)** para el robot humanoide **Unitree G1**, permitiendo que el robot ejecute tareas mediante instrucciones en lenguaje natural sin entrenamiento espec칤fico para cada tarea. La soluci칩n integra representaciones sem치nticas del lenguaje con control rob칩tico en simulaci칩n (MuJoCo) y hardware real.

## 丘뙖잺 쯈u칠 es CLIP y por qu칠 es clave para este proyecto?

**CLIP (Contrastive Language-Image Pretraining)** es un modelo de inteligencia artificial desarrollado por OpenAI que aprende asociaciones entre im치genes y texto durante su entrenamiento. Es fundamental para nuestro proyecto por estas razones:

### 游댌 쮺칩mo funciona CLIP?
- **Entrenamiento contrastivo**: Aprende a asociar im치genes con sus descripciones de texto en un espacio vectorial compartido
- **Dos componentes principales**:
  1. **Image Encoder**: Convierte im치genes en vectores (embeddings)
  2. **Text Encoder**: Convierte texto en vectores del mismo espacio
- **Cero disparo (zero-shot)**: Puede clasificar im치genes en categor칤as no vistas durante el entrenamiento

### 游눠 Ejemplo pr치ctico en nuestro proyecto:
```python
import clip
import torch

# Cargar modelo preentrenado
model, preprocess = clip.load("ViT-B/32")

# Convertir comando de voz a vector
command = "camina hacia el objeto rojo"
text_input = clip.tokenize([command]).to(device)
text_features = model.encode_text(text_input)

# Convertir imagen de la c치mara del robot a vector
image = preprocess(pil_image).unsqueeze(0).to(device)
image_features = model.encode_image(image)

# Calcular similitud entre comando e imagen
similarity = (100.0 * text_features @ image_features.T).softmax(dim=-1)